mockingjay:
  downsample_rate: 3                                    # stacked consecutive features vectors to reduce the length of input sequences by this factor.
  hidden_size: 768                                      # Size of the encoder layers and the pooler layer.
  num_hidden_layers: 12                                 # Number of hidden layers in the Transformer encoder.
  num_attention_heads: 12                               # Number of attention heads for each attention layer in the Transformer encoder.
  intermediate_size: 3072                               # The size of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
  hidden_act: "gelu"                                    # The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu", "relu" and "swish" are supported.
  hidden_dropout_prob: 0.1                              # The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.
  attention_probs_dropout_prob: 0.1                     # The dropout ratio for the attention probabilities.
  initializer_range: 0.02                               # The sttdev of the truncated_normal_initializer for initializing all weight matrices.
  layer_norm_eps: "1e-12"                               # The epsilon used by LayerNorm.


optimizer: 
  learning_rate: "4e-4"                                 # Learning rate for opt
  loss_scale: 0                                         # Loss scale to improve fp16 numeric stability. Only used when apex is set to True. 0: dynamic loss scaling. positive power of 2: static loss scaling.
  warmup_proportion: 0.07                               # Proportion of training to perform linear rate warmup.
  gradient_accumulation_steps: 1                        # Number of updates steps to accumulate before performing a backward/update pass
  gradient_clipping: 1.0                                # Maximum gradient norm


dataloader:
  n_jobs: 16                                            # Subprocess used for torch Dataloader
  batch_size: 6                                         # training batch size
  dev_batch_size: 6                                     # used for dev/test splits
  max_timestep: 3000                                    # Max length for audio feature (0 for no restriction)
  max_label_len: 400                                    # Max length for output sequence (0 for no restriction)

  # LIBRISEECH SETTINGS
  data_path: 'data/libri_mel160_subword5000'            # Source data path
  target_path: 'data/libri_linear1025_subword5000'      # Target data path for reconstruction to a different feature type, set dataset to 'librispeech_mel_linear'.
  phone_path: 'data/libri_phone'                        # phone boundary label data path for the phone classification task.
  train_set: ['train-clean-360']                        #
  dev_set: ['dev-clean']                                #
  test_set: ['test-clean']                              #
  train_proportion: 1.0                                 # Currently only effect the `phone classification task`, use this percent of `train_set` for downstream task training to demonstrate mockingjay generality

  # MOSI SETTINGS
  sentiment_path: 'data/mosi_nonstandard_mel160'                    # train/dev/test set has fixed subdir name with 'test', 'dev', 'test'


solver:
  # Training options
  apex: False                                           # Use APEX (see https://github.com/NVIDIA/apex for more details)
  total_steps: 500000                                   # total steps for training, a step is a batch of update
  log_step: 100                                         # log training status every this amount of training steps
  mask_proportion: 0.15                                 # mask this percentage of all spectrogram frames in each sequence at random during MAM training                        
  dev_step: 2000                                        #
  duo_feature: True                                     # Use different input / output features during training

  # models
  load_model_list: ['SpecHead', 'Mockingjay']           # load the components in the list for test/eval
  max_keep: 10                                          # maximum number of model ckpt to keep during training
  save_step: 1000                                       # save model every this amount of training steps


downstream:                                             # downstream model config
  linear:
    input_dim: 'None'                                     # `int`, else if set to None, input_dim will be set according to mockingjay settings or mel-preprocessing dimensions automatically
    hidden_size: 768
    drop: 0.2
    select_hidden: 'last'                                 # support modes: 'last', 'first', 'average'

  rnn:
    input_dim: 'None'                                     # `int`, else if set to None, input_dim will be set according to mockingjay settings or mel-preprocessing dimensions automatically
    hidden_size: 768
    drop: 0.2
    select_hidden: 'last'                                 # support modes: 'last', 'first', 'average'
    use_linear: True


  # Training options
  learning_rate: "4e-3"                                 # Learning rate for opt
  total_steps: 500000                                   # total steps for training, a step is a batch of update
  log_step: 1000                                        # log training status every this amount of training steps
  evaluation: 'test'                                    # can be 'dev' or 'test', show inference results right after saving model
  
  # models
  load_model_list: ['Classifier']                       # load the components in the list for test/eval
  max_keep: 10                                          # maximum number of model ckpt to keep during training
  save_step: 1000                                       # save model every this amount of training steps

