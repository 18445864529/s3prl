mockingjay:
  downsample_rate: 3                          # stacked consecutive features vectors to reduce the length of input sequences by this factor.
  hidden_size: 768                            # Size of the encoder layers and the pooler layer.
  num_hidden_layers: 12                       # Number of hidden layers in the Transformer encoder.
  num_attention_heads: 12                     # Number of attention heads for each attention layer in the Transformer encoder.
  intermediate_size: 3072                     # The size of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
  hidden_act: "gelu"                          # The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu", "relu" and "swish" are supported.
  hidden_dropout_prob: 0.1                    # The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.
  attention_probs_dropout_prob: 0.1           # The dropout ratio for the attention probabilities.
  initializer_range: 0.02                     # The sttdev of the truncated_normal_initializer for initializing all weight matrices.
  layer_norm_eps: "1e-12"                     # The epsilon used by LayerNorm.

optimizer: 
  learning_rate: "5e-5"                     # Learning rate for opt
  loss_scale: 0                             # Loss scale to improve fp16 numeric stability. Only used when apex is set to True. 0: dynamic loss scaling. positive power of 2: static loss scaling.
  warmup_proportion: 0.1                    # Proportion of training to perform linear rate warmup.
  gradient_accumulation_steps: 1            # Number of updates steps to accumulate before performing a backward/update pass

solver:
  # Data options
  dataset: 'librispeech'                      # 
  data_path: 'data/libri_fbank80_subword5000' # Source data path
  n_jobs: 8                                   # Subprocess used for torch Dataloader
  max_timestep: 3000                          # Max length for audio feature (0 for no restriction)
  max_label_len: 400                          # Max length for output sequence (0 for no restriction)
  # Training options
  apex: False                                 # Use APEX (see https://github.com/NVIDIA/apex for more details)
  train_set: ['train-clean-360']              #
  batch_size: 16                              # training batch size
  total_epochs: 1000                          # total epochs for training                         
  tf_start: 1.0                               # teacher forcing rate during training will be linearly
  tf_end: 1.0                                 # decaying from upperbound to lower bound for each epoch
  # Validation options
  dev_set: ['dev-clean']                      
  dev_batch_size: 4                             
  dev_step: 2000
  # Decoding options
  test_set: ['test-clean']
  max_decode_step_ratio: 0.1
  decode_beam_size: 10
  decode_ctc_weight: 0.5
  decode_lm_path: 'result/libri_rnnlm460_sd0/rnnlm'
  decode_lm_weight: 0.5

